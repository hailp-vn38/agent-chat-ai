import asyncio
import logging
import os
import statistics
import time
import concurrent.futures
from typing import Dict, Optional
import aiohttp
from tabulate import tabulate
from core.utils.llm import create_instance as create_llm_instance
from config.settings import load_config

# ƒê·∫∑t m·ª©c log to√†n c·ª•c l√† WARNING ƒë·ªÉ h·∫°n ch·∫ø log INFO
logging.basicConfig(level=logging.WARNING)

description = "B√†i ki·ªÉm tra hi·ªáu nƒÉng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn"


class LLMPerformanceTester:
    def __init__(self):
        self.config = load_config()
        # S·ª≠ d·ª•ng b·ªô n·ªôi dung ki·ªÉm th·ª≠ ph√π h·ª£p v·ªõi b·ªëi c·∫£nh agent, k√®m prompt h·ªá th·ªëng
        self.system_prompt = self._load_system_prompt()
        self.test_sentences = self.config.get("module_test", {}).get(
            "test_sentences",
            [
                "Ch√†o b·∫°n, h√¥m nay t√¥i h∆°i bu·ªìn, b·∫°n c√≥ th·ªÉ an ·ªßi t√¥i kh√¥ng?",
                "B·∫°n gi√∫p t√¥i xem th·ªùi ti·∫øt ng√†y mai nh∆∞ th·∫ø n√†o ƒë∆∞·ª£c kh√¥ng?",
                "T√¥i mu·ªën nghe m·ªôt c√¢u chuy·ªán th√∫ v·ªã, b·∫°n c√≥ th·ªÉ k·ªÉ cho t√¥i ch·ª©?",
                "B√¢y gi·ªù l√† m·∫•y gi·ªù? H√¥m nay l√† th·ª© m·∫•y?",
                "T√¥i mu·ªën ƒë·∫∑t b√°o th·ª©c l√∫c 8 gi·ªù s√°ng mai ƒë·ªÉ nh·∫Øc t√¥i h·ªçp.",
            ],
        )
        self.results = {}

    def _load_system_prompt(self) -> str:
        """N·∫°p prompt h·ªá th·ªëng"""
        try:
            prompt_file = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), "agent-base-prompt.txt"
            )
            with open(prompt_file, "r", encoding="utf-8") as f:
                content = f.read()
                # Thay th·∫ø bi·∫øn trong template b·∫±ng gi√° tr·ªã ph·ª•c v·ª• ki·ªÉm th·ª≠
                content = content.replace(
                    "{{base_prompt}}",
                    "B·∫°n l√† Ti·ªÉu Tr√≠, m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† d·ªÖ m·∫øn",
                )
                content = content.replace(
                    "{{emojiList}}", "üòÄ,üòÉ,üòÑ,üòÅ,üòä,üòç,ü§î,üòÆ,üò±,üò¢,üò≠,üò¥,üòµ,ü§ó,üôÑ"
                )
                content = content.replace("{{current_time}}", "17/08/2024 12:30:45")
                content = content.replace("{{today_date}}", "17/08/2024")
                content = content.replace("{{today_weekday}}", "Th·ª© B·∫£y")
                content = content.replace("{{local_address}}", "B·∫Øc Kinh")
                content = content.replace("{{weather_info}}", "H√¥m nay tr·ªùi n·∫Øng, 25-32‚ÑÉ")
                return content
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ t·∫£i t·ªáp prompt h·ªá th·ªëng: {e}")
            return "B·∫°n l√† Ti·ªÉu Tr√≠, m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† d·ªÖ m·∫øn. H√£y tr·∫£ l·ªùi ng∆∞·ªùi d√πng b·∫±ng gi·ªçng ƒëi·ªáu ·∫•m √°p, th√¢n thi·ªán."

    def _collect_response_sync(self, llm, messages, llm_name, sentence_start):
        """H√†m h·ªó tr·ª£ thu th·∫≠p ph·∫£n h·ªìi ƒë·ªìng b·ªô"""
        chunks = []
        first_token_received = False
        first_token_time = None

        try:
            response_generator = llm.response("perf_test", messages)
            chunk_count = 0
            for chunk in response_generator:
                chunk_count += 1
                # Sau m·ªói s·ªë l∆∞·ª£ng chunk nh·∫•t ƒë·ªãnh th√¨ ki·ªÉm tra xem c√≥ c·∫ßn d·ª´ng hay kh√¥ng
                if chunk_count % 10 == 0:
                    # Ki·ªÉm tra thread hi·ªán t·∫°i c√≥ b·ªã ƒë√°nh d·∫•u d·ª´ng hay ch∆∞a
                    import threading

                    if (
                        threading.current_thread().ident
                        != threading.main_thread().ident
                    ):
                        # N·∫øu kh√¥ng ph·∫£i main thread, ki·ªÉm tra xem c√≥ n√™n d·ª´ng
                        pass

                # Ki·ªÉm tra chunk c√≥ ch·ª©a th√¥ng tin l·ªói hay kh√¥ng
                chunk_str = str(chunk)
                if (
                    "exception" in chunk_str.lower()
                    or "error" in chunk_str.lower()
                    or "502" in chunk_str.lower()
                    or "b·∫•t th∆∞·ªùng" in chunk_str.lower()
                    or "l·ªói" in chunk_str.lower()
                ):
                    error_msg = chunk_str.lower()
                    print(f"{llm_name} ph·∫£n h·ªìi ch·ª©a l·ªói: {error_msg}")
                    # N√©m ngo·∫°i l·ªá v·ªõi th√¥ng tin l·ªói
                    raise Exception(chunk_str)

                if not first_token_received and chunk.strip() != "":
                    first_token_time = time.time() - sentence_start
                    first_token_received = True
                    print(f"{llm_name} token ƒë·∫ßu ti√™n: {first_token_time:.3f}s")
                chunks.append(chunk)
        except Exception as e:
            # Ghi l·∫°i th√¥ng tin l·ªói chi ti·∫øt h∆°n
            error_msg = str(e).lower()
            print(f"{llm_name} g·∫∑p l·ªói khi thu th·∫≠p ph·∫£n h·ªìi: {error_msg}")
            # V·ªõi l·ªói 502 ho·∫∑c l·ªói m·∫°ng, n√©m ngo·∫°i l·ªá cho l·ªõp tr√™n x·ª≠ l√Ω
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
                or "b·∫•t th∆∞·ªùng" in str(e).lower()
                or "l·ªói" in str(e).lower()
            ):
                raise e
            # V·ªõi l·ªói kh√°c c√≥ th·ªÉ tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ng ph·∫ßn
            return chunks, first_token_time

        return chunks, first_token_time

    async def _check_ollama_service(self, base_url: str, model_name: str) -> bool:
        """Ki·ªÉm tra tr·∫°ng th√°i d·ªãch v·ª• Ollama theo c√°ch b·∫•t ƒë·ªìng b·ªô"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{base_url}/api/version") as response:
                    if response.status != 200:
                        print(f"D·ªãch v·ª• Ollama ch∆∞a kh·ªüi ƒë·ªông ho·∫∑c kh√¥ng truy c·∫≠p ƒë∆∞·ª£c: {base_url}")
                        return False
                async with session.get(f"{base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get("models", [])
                        if not any(model["name"] == model_name for model in models):
                            print(
                                f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh Ollama {model_name}, h√£y ch·∫°y `ollama pull {model_name}` tr∆∞·ªõc"
                            )
                            return False
                    else:
                        print("Kh√¥ng th·ªÉ l·∫•y danh s√°ch m√¥ h√¨nh Ollama")
                        return False
                return True
            except Exception as e:
                print(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi d·ªãch v·ª• Ollama: {str(e)}")
                return False

    async def _test_single_sentence(
        self, llm_name: str, llm, sentence: str
    ) -> Optional[Dict]:
        """ƒêo hi·ªáu nƒÉng v·ªõi m·ªôt c√¢u h·ªèi"""
        try:
            print(f"{llm_name} b·∫Øt ƒë·∫ßu ki·ªÉm th·ª≠: {sentence[:20]}...")
            sentence_start = time.time()
            first_token_received = False
            first_token_time = None

            # X√¢y d·ª±ng th√¥ng ƒëi·ªáp c√≥ prompt h·ªá th·ªëng
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": sentence},
            ]

            # D√πng asyncio.wait_for ƒë·ªÉ ki·ªÉm so√°t timeout
            try:
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # T·∫°o t√°c v·ª• thu th·∫≠p ph·∫£n h·ªìi
                    future = executor.submit(
                        self._collect_response_sync,
                        llm,
                        messages,
                        llm_name,
                        sentence_start,
                    )

                    # D√πng asyncio.wait_for ƒë·ªÉ √°p timeout
                    try:
                        response_chunks, first_token_time = await asyncio.wait_for(
                            asyncio.wrap_future(future), timeout=10.0
                        )
                    except asyncio.TimeoutError:
                        print(f"{llm_name} ki·ªÉm th·ª≠ qu√° th·ªùi gian (10 gi√¢y), b·ªè qua")
                        # H·ªßy future b·∫Øt bu·ªôc
                        future.cancel()
                        # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ thread pool ph·∫£n h·ªìi vi·ªác h·ªßy
                        try:
                            await asyncio.wait_for(
                                asyncio.wrap_future(future), timeout=1.0
                            )
                        except (
                            asyncio.TimeoutError,
                            concurrent.futures.CancelledError,
                            Exception,
                        ):
                            # B·ªè qua m·ªçi ngo·∫°i l·ªá ƒë·ªÉ ch∆∞∆°ng tr√¨nh ti·∫øp t·ª•c ch·∫°y
                            pass
                        return None

            except Exception as timeout_error:
                print(f"{llm_name} g·∫∑p l·ªói khi x·ª≠ l√Ω: {timeout_error}")
                return None

            response_time = time.time() - sentence_start
            print(f"{llm_name} ho√†n th√†nh ph·∫£n h·ªìi: {response_time:.3f}s")

            return {
                "name": llm_name,
                "type": "llm",
                "first_token_time": first_token_time,
                "response_time": response_time,
            }
        except Exception as e:
            error_msg = str(e).lower()
            # Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói 502 ho·∫∑c l·ªói m·∫°ng hay kh√¥ng
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
            ):
                print(f"{llm_name} g·∫∑p l·ªói 502, b·ªè qua ki·ªÉm th·ª≠")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng 502",
                }
            print(f"{llm_name} ki·ªÉm th·ª≠ c√¢u th·∫•t b·∫°i: {str(e)}")
            return None

    async def _test_llm(self, llm_name: str, config: Dict) -> Dict:
        """Ki·ªÉm tra b·∫•t ƒë·ªìng b·ªô hi·ªáu nƒÉng t·ª´ng LLM"""
        try:
            # V·ªõi Ollama, b·ªè qua ki·ªÉm tra api_key v√† x·ª≠ l√Ω ƒë·∫∑c bi·ªát
            if llm_name == "Ollama":
                base_url = config.get("base_url", "http://localhost:11434")
                model_name = config.get("model_name")
                if not model_name:
                    print("Ollama ch∆∞a c·∫•u h√¨nh model_name")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói m·∫°ng",
                    }

                if not await self._check_ollama_service(base_url, model_name):
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói m·∫°ng",
                    }
            else:
                if "api_key" in config and any(
                    x in config["api_key"] for x in ["‰Ω†ÁöÑ", "placeholder", "sk-xxx"]
                ):
                    print(f"B·ªè qua LLM ch∆∞a c·∫•u h√¨nh: {llm_name}")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói c·∫•u h√¨nh",
                    }

            # L·∫•y ki·ªÉu th·ª±c t·∫ø (t∆∞∆°ng th√≠ch c·∫•u h√¨nh c≈©)
            module_type = config.get("type", llm_name)
            llm = create_llm_instance(module_type, config)

            # ƒê·ªìng b·ªô s·ª≠ d·ª•ng UTF-8
            test_sentences = [
                s.encode("utf-8").decode("utf-8") for s in self.test_sentences
            ]

            # T·∫°o t√°c v·ª• ki·ªÉm th·ª≠ cho t·ª´ng c√¢u
            sentence_tasks = []
            for sentence in test_sentences:
                sentence_tasks.append(
                    self._test_single_sentence(llm_name, llm, sentence)
                )

            # Th·ª±c thi song song c√°c ki·ªÉm th·ª≠ c√¢u v√† x·ª≠ l√Ω ngo·∫°i l·ªá
            sentence_results = await asyncio.gather(
                *sentence_tasks, return_exceptions=True
            )

            # X·ª≠ l√Ω k·∫øt qu·∫£, lo·∫°i b·ªè ngo·∫°i l·ªá v√† gi√° tr·ªã None
            valid_results = []
            for result in sentence_results:
                if isinstance(result, dict) and result is not None:
                    valid_results.append(result)
                elif isinstance(result, Exception):
                    error_msg = str(result).lower()
                    if "502" in error_msg or "bad gateway" in error_msg:
                        print(f"{llm_name} g·∫∑p l·ªói 502, b·ªè qua c√¢u ki·ªÉm th·ª≠ n√†y")
                        return {
                            "name": llm_name,
                            "type": "llm",
                            "errors": 1,
                            "error_type": "L·ªói m·∫°ng 502",
                        }
                    else:
                        print(f"{llm_name} c√¢u ki·ªÉm th·ª≠ g·∫∑p ngo·∫°i l·ªá: {result}")

            if not valid_results:
                print(f"{llm_name} kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá, c√≥ th·ªÉ do l·ªói m·∫°ng ho·∫∑c c·∫•u h√¨nh")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

            # Ki·ªÉm tra s·ªë l∆∞·ª£ng k·∫øt qu·∫£ h·ª£p l·ªá, qu√° √≠t th√¨ xem l√† th·∫•t b·∫°i
            if len(valid_results) < len(test_sentences) * 0.3:  # C·∫ßn √≠t nh·∫•t 30% th√†nh c√¥ng
                print(
                    f"{llm_name} c√≥ qu√° √≠t c√¢u ki·ªÉm th·ª≠ th√†nh c√¥ng ({len(valid_results)}/{len(test_sentences)}), c√≥ th·ªÉ m·∫°ng kh√¥ng ·ªïn ƒë·ªãnh ho·∫∑c API g·∫∑p v·∫•n ƒë·ªÅ"
                )
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

            first_token_times = [
                r["first_token_time"]
                for r in valid_results
                if r.get("first_token_time")
            ]
            response_times = [r["response_time"] for r in valid_results]

            # L·ªçc b·ªè d·ªØ li·ªáu b·∫•t th∆∞·ªùng (l·ªõn h∆°n 3 ƒë·ªô l·ªách chu·∫©n)
            if len(response_times) > 1:
                mean = statistics.mean(response_times)
                stdev = statistics.stdev(response_times)
                filtered_times = [t for t in response_times if t <= mean + 3 * stdev]
            else:
                filtered_times = response_times

            return {
                "name": llm_name,
                "type": "llm",
                "avg_response": sum(response_times) / len(response_times),
                "avg_first_token": (
                    sum(first_token_times) / len(first_token_times)
                    if first_token_times
                    else 0
                ),
                "success_rate": f"{len(valid_results)}/{len(test_sentences)}",
                "errors": 0,
            }
        except Exception as e:
            error_msg = str(e).lower()
            if "502" in error_msg or "bad gateway" in error_msg:
                print(f"LLM {llm_name} g·∫∑p l·ªói 502, b·ªè qua ki·ªÉm th·ª≠")
            else:
                print(f"LLM {llm_name} ki·ªÉm th·ª≠ th·∫•t b·∫°i: {str(e)}")
            error_type = "L·ªói m·∫°ng"
            if "timeout" in str(e).lower():
                error_type = "K·∫øt n·ªëi qu√° th·ªùi gian"
            return {
                "name": llm_name,
                "type": "llm",
                "errors": 1,
                "error_type": error_type,
            }

    def _print_results(self):
        """In k·∫øt qu·∫£ ki·ªÉm th·ª≠"""
        print("\n" + "=" * 50)
        print("K·∫øt qu·∫£ ki·ªÉm tra hi·ªáu nƒÉng LLM")
        print("=" * 50)

        if not self.results:
            print("Kh√¥ng c√≥ k·∫øt qu·∫£ ki·ªÉm th·ª≠ kh·∫£ d·ª•ng")
            return

        headers = ["T√™n m√¥ h√¨nh", "Th·ªùi gian ph·∫£n h·ªìi TB(s)", "Th·ªùi gian token ƒë·∫ßu(s)", "T·ªâ l·ªá th√†nh c√¥ng", "Tr·∫°ng th√°i"]
        table_data = []

        # Thu th·∫≠p v√† ph√¢n lo·∫°i d·ªØ li·ªáu
        valid_results = []
        error_results = []

        for name, data in self.results.items():
            if data["errors"] == 0:
                # K·∫øt qu·∫£ h·ª£p l·ªá
                avg_response = f"{data['avg_response']:.3f}"
                avg_first_token = (
                    f"{data['avg_first_token']:.3f}"
                    if data["avg_first_token"] > 0
                    else "-"
                )
                success_rate = data.get("success_rate", "N/A")
                status = "‚úÖ B√¨nh th∆∞·ªùng"

                # L∆∞u gi√° tr·ªã ph·ª•c v·ª• vi·ªác s·∫Øp x·∫øp
                first_token_value = (
                    data["avg_first_token"]
                    if data["avg_first_token"] > 0
                    else float("inf")
                )

                valid_results.append(
                    {
                        "name": name,
                        "avg_response": avg_response,
                        "avg_first_token": avg_first_token,
                        "success_rate": success_rate,
                        "status": status,
                        "sort_key": first_token_value,
                    }
                )
            else:
                # K·∫øt qu·∫£ l·ªói
                avg_response = "-"
                avg_first_token = "-"
                success_rate = "0/5"

                # L·∫•y lo·∫°i l·ªói c·ª• th·ªÉ
                error_type = data.get("error_type", "L·ªói m·∫°ng")
                status = f"‚ùå {error_type}"

                error_results.append(
                    [name, avg_response, avg_first_token, success_rate, status]
                )

        # S·∫Øp x·∫øp theo th·ªùi gian token ƒë·∫ßu tƒÉng d·∫ßn
        valid_results.sort(key=lambda x: x["sort_key"])

        # Chuy·ªÉn k·∫øt qu·∫£ h·ª£p l·ªá ƒë√£ s·∫Øp x·∫øp sang d·ªØ li·ªáu b·∫£ng
        for result in valid_results:
            table_data.append(
                [
                    result["name"],
                    result["avg_response"],
                    result["avg_first_token"],
                    result["success_rate"],
                    result["status"],
                ]
            )

        # ƒê∆∞a c√°c b·∫£n ghi l·ªói v√†o cu·ªëi b·∫£ng
        table_data.extend(error_results)

        print(tabulate(table_data, headers=headers, tablefmt="grid"))
        print("\nGhi ch√∫ ki·ªÉm th·ª≠:")
        print("- N·ªôi dung ki·ªÉm th·ª≠: k·ªãch b·∫£n h·ªôi tho·∫°i agent v·ªõi prompt h·ªá th·ªëng ƒë·∫ßy ƒë·ªß")
        print("- Ki·ªÉm so√°t timeout: t·ªëi ƒëa 10 gi√¢y cho m·ªói y√™u c·∫ßu")
        print("- X·ª≠ l√Ω l·ªói: t·ª± ƒë·ªông b·ªè qua m√¥ h√¨nh g·∫∑p l·ªói 502 ho·∫∑c s·ª± c·ªë m·∫°ng")
        print("- T·ªâ l·ªá th√†nh c√¥ng: s·ªë c√¢u ph·∫£n h·ªìi th√†nh c√¥ng / t·ªïng s·ªë c√¢u ki·ªÉm th·ª≠")
        print("\nKi·ªÉm th·ª≠ ho√†n t·∫•t!")

    async def run(self):
        """Ch·∫°y to√†n b·ªô b√†i ki·ªÉm tra b·∫•t ƒë·ªìng b·ªô"""
        print("B·∫Øt ƒë·∫ßu l·ªçc c√°c module LLM kh·∫£ d·ª•ng...")

        # T·∫°o to√†n b·ªô t√°c v·ª• ki·ªÉm th·ª≠
        all_tasks = []

        # T√°c v·ª• ki·ªÉm th·ª≠ LLM
        if self.config.get("LLM") is not None:
            for llm_name, config in self.config.get("LLM", {}).items():
                # Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa c·∫•u h√¨nh
                if llm_name == "CozeLLM":
                    if any(x in config.get("bot_id", "") for x in ["‰Ω†ÁöÑ"]) or any(
                        x in config.get("user_id", "") for x in ["‰Ω†ÁöÑ"]
                    ):
                        print(f"LLM {llm_name} ch∆∞a c·∫•u h√¨nh bot_id/user_id, b·ªè qua")
                        continue
                elif "api_key" in config and any(
                    x in config["api_key"] for x in ["‰Ω†ÁöÑ", "placeholder", "sk-xxx"]
                ):
                    print(f"LLM {llm_name} ch∆∞a c·∫•u h√¨nh api_key, b·ªè qua")
                    continue

                # V·ªõi Ollama, ki·ªÉm tra tr·∫°ng th√°i d·ªãch v·ª• tr∆∞·ªõc
                if llm_name == "Ollama":
                    base_url = config.get("base_url", "http://localhost:11434")
                    model_name = config.get("model_name")
                    if not model_name:
                        print("Ollama ch∆∞a c·∫•u h√¨nh model_name")
                        continue

                    if not await self._check_ollama_service(base_url, model_name):
                        continue

                print(f"Th√™m t√°c v·ª• ki·ªÉm tra LLM: {llm_name}")
                all_tasks.append(self._test_llm(llm_name, config))

        print(f"\nT√¨m th·∫•y {len(all_tasks)} module LLM kh·∫£ d·ª•ng")
        print("\nB·∫Øt ƒë·∫ßu ki·ªÉm tra song song t·∫•t c·∫£ module...\n")

        # Th·ª±c thi song song m·ªçi t√°c v·ª• v√† ƒë·∫∑t timeout ri√™ng cho t·ª´ng t√°c v·ª•
        async def test_with_timeout(task, timeout=30):
            """Th√™m c∆° ch·∫ø timeout b·∫£o v·ªá cho t·ª´ng t√°c v·ª• ki·ªÉm th·ª≠"""
            try:
                return await asyncio.wait_for(task, timeout=timeout)
            except asyncio.TimeoutError:
                print(f"T√°c v·ª• ki·ªÉm th·ª≠ qu√° th·ªùi gian ({timeout} gi√¢y), b·ªè qua")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "K·∫øt n·ªëi qu√° th·ªùi gian",
                }
            except Exception as e:
                print(f"T√°c v·ª• ki·ªÉm th·ª≠ g·∫∑p ngo·∫°i l·ªá: {str(e)}")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

        # Bao b·ªçc timeout b·∫£o v·ªá cho t·ª´ng t√°c v·ª•
        protected_tasks = [test_with_timeout(task) for task in all_tasks]

        # Th·ª±c thi song song m·ªçi t√°c v·ª• ki·ªÉm th·ª≠
        all_results = await asyncio.gather(*protected_tasks, return_exceptions=True)

        # X·ª≠ l√Ω k·∫øt qu·∫£
        for result in all_results:
            if isinstance(result, dict):
                if result.get("errors") == 0:
                    self.results[result["name"]] = result
                else:
                    # V·∫´n ghi l·∫°i l·ªói ƒë·ªÉ hi·ªÉn th·ªã tr·∫°ng th√°i th·∫•t b·∫°i
                    if result.get("name") != "Unknown":
                        self.results[result["name"]] = result
            elif isinstance(result, Exception):
                print(f"L·ªói khi x·ª≠ l√Ω k·∫øt qu·∫£ ki·ªÉm th·ª≠: {str(result)}")

        # In k·∫øt qu·∫£
        print("\nT·∫°o b√°o c√°o ki·ªÉm th·ª≠...")
        self._print_results()


async def main():
    tester = LLMPerformanceTester()
    await tester.run()


if __name__ == "__main__":
    asyncio.run(main())
